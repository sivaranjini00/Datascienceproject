## ğŸ“Š Data Science Project


## ğŸ“Œ Project Overview

This project is an end-to-end Machine Learning pipeline designed using a modular architecture.
It automates the complete ML lifecycle from data collection to model training and evaluation, while ensuring trackability, scalability, and reproducibility.

The pipeline is built following MLOps best practices and integrates with MLflow and DagsHub for experiment tracking.

## âœ¨ Key Highlights

ğŸ§  End-to-End ML Workflow â€” Data ingestion â†’ validation â†’ transformation â†’ training â†’ evaluation

âš¡ Modular Code Structure â€” Separate reusable components for each pipeline stage

âš™ï¸ Config-Driven Approach â€” Controlled using config.yaml, schema.yaml, and params.yaml

ğŸ“‚ Artifacts Management â€” Intermediate outputs stored in dedicated artifact folders

ğŸ“ˆ Experiment Tracking â€” MLflow integrated with DagsHub for model versioning and metric logging

ğŸ“¦ Reproducible Pipelines â€” Easy to reproduce runs using main.py

ğŸ§ª Test/Train Split & Evaluation Metrics â€” Standard evaluation pipeline for comparing models

## ğŸ—‚ Project Structure
'''text
Datascienceproject/
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ schema.yaml
â”‚   â””â”€â”€ params.yaml
â”‚
â”œâ”€â”€ src/
â”‚   â””â”€â”€ Datascienceproject/
â”‚       â”œâ”€â”€ components/           # Data Ingestion, Validation, Transformation, Training, Evaluation logic
â”‚       â”œâ”€â”€ config/               # Configuration manager classes
â”‚       â”œâ”€â”€ entity/                # Entity dataclasses for structured configs
â”‚       â”œâ”€â”€ pipeline/              # Orchestrating pipelines for each stage
â”‚       â”œâ”€â”€ utils/                  # Utility/helper functions
â”‚       â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ artifacts/                     # Generated data, models, and reports
â”‚
â”œâ”€â”€ main.py                         # Entry point to run full pipeline
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
